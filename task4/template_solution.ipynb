{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57002395eb6b50f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 4\n",
    "This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n",
    "This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a46aca0e5d9ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, we import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c6ff7632991155",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add any other imports you need here\n",
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cba49",
   "metadata": {},
   "source": [
    "Depending on your approach, you might need to adapt the structure of this template or parts not marked by TODOs.\n",
    "It is not necessary to completely follow this template. Feel free to add more code and delete any parts that are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9b5e89d4b3a02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32  # TODO: Set the batch size according to both training performance and available memory\n",
    "NUM_EPOCHS = 5  # TODO: Set the number of epochs\n",
    "\n",
    "train_val = pd.read_csv(\"train.csv\")\n",
    "test_val = pd.read_csv(\"test_no_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161fdafaedaa5b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fill out ReviewDataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data_frame, tokenizer=GPT2Tokenizer.from_pretrained('gpt2'), max_length=512):\n",
    "        self.data_frame = data_frame\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.titles = data_frame['title'].values\n",
    "        self.sentences = data_frame['sentence'].values\n",
    "        self.scores = data_frame['score'].values if 'score' in data_frame.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.titles[index]\n",
    "        sentence = self.sentences[index]\n",
    "        text = title + \" \" + sentence\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "        if self.scores is not None:\n",
    "            score = self.scores[index]\n",
    "            sample['score'] = torch.tensor(score, dtype=torch.float)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "676f7a012f50e988",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programme\\anaconda\\envs\\IML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ReviewDataset(train_val)\n",
    "test_dataset = ReviewDataset(test_val)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, num_workers=16, pin_memory=True)\n",
    "# Additional code if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843f38b9dbea00b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programme\\anaconda\\envs\\IML\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fill out MyModule\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.config = GPT2Config.from_pretrained('gpt2')\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1)   #Only 1 output value\n",
    "\n",
    "    def forward(self, sample):\n",
    "        \n",
    "        input_ids = sample['input_ids']\n",
    "        attention_mask = sample['attention_mask']\n",
    "        \n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = hidden_states[:, -1, :]  # (batch_size, hidden_size)\n",
    "        score = self.regression_head(pooled_output)\n",
    "        return score\n",
    "\n",
    "\n",
    "model = MyModule().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "605e5bd0373a1dda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# TODO: Setup loss function, optimiser, and scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = MSELoss()\n",
    "scheduler = None\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # TODO: Set up training loop\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        scores = batch['score'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), scores)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e56bba5fa2d905",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = []\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # TODO: Set up evaluation loop\n",
    "\n",
    "    with open(\"result.txt\", \"w\") as f:\n",
    "        for val in np.concatenate(results):\n",
    "            f.write(f\"{val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
